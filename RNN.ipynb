{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical description\n",
    "<img src=\"images/RNN1.png\" width=\"500\"><br>\n",
    "\n",
    "Credits: Udacity Computer Vision Nanodegree<br><br>\n",
    "\n",
    "Reccurent neural networks' output depends not only on current input like in for example CNN, but also on states which emulates \"memory\". There are two main differences between RNN's and Feed Forward neural Networks(FFNN):<br>\n",
    "1. In RNN output of hidden layer becomes input to the network in the next training step.<br>\n",
    "2. In RNN, input is not a single example from dataset but a sequence.<br>\n",
    "\n",
    "<img src=\"images/RNN2.png\" width=\"500\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "RNN model can be presened in **folded** version as in every single timestep it will look the same.\n",
    "\n",
    "<img src=\"images/RNN4.png\" width=\"500\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "\n",
    "Also **unfolded** version is useful to see more clearly how does \"memory\" component looks like.<br>\n",
    "Here we can see that for example output at timestep $\\bar{y}_{t+2}$ depends on the current input(the same timestep) and all previous inputs which are treated as memory element.\n",
    "\n",
    "<img src=\"images/RNN3.png\" width=\"500\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "Generally it can be seen that in RNN, output $\\bar{y}_t$ at time $t$ can be expressed as a function of previous inputs and weights: $\\bar{y}_t=F(\\bar{x}_{t},\\bar{x}_{t-1},\\bar{x}_{t-2},\\bar{x}_{t-3},,...,\\bar{x}_{t-t_{0}},W)$<br>\n",
    "and state layer $\\bar{s}_{t}$:<br>\n",
    "$\\bar{s}_{t}=\\Phi{(\\bar{x}_{t}W_{x}+\\bar{s}_{t-1}W_{s})}$, where $\\Phi$ is an activation function.<br>\n",
    "Finally, we can compute output vector and it is the same as in FFNN:<br>\n",
    "$\\bar{y}_{t}=\\bar{s}_{t}W_{y}$<br>\n",
    "\n",
    "### Unfolded Model\n",
    "To understand the nature of the **unfolded model** first let's take a look at **Elman Network** at time $t$.<br>\n",
    "\n",
    "\n",
    "At time $t$ | At time $t+1$\n",
    "- | - \n",
    "![alt](images/RNN5.png) | ![alt](images/RNN6.png)\n",
    "\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "Above **Elman Network** with 1 hidden layer is presented. At any given time $t$, we have first part of input vector $\\bar{x}_{t}$ connected to the hidden layer $\\bar{s}_{t}$ with weight matrix $W_{x}$ and second part $\\bar{s}_{t-1}$ which is the hidden layer from time $t-1$ connected also to he hidden layer $\\bar{s}_{t}$, but with weight matrix $W_{s}$. Then $\\bar{s}_{t}$ is connected to the output layer $\\bar{y}_{t}$ with weight matrix $W_y$.<br>\n",
    "\n",
    "At time $t+1$, it can be seen that the structure of the system is the same. Again, we take input vector, this time $\\bar{x}_{t+1}$ and as that second part of our input we take hidden layer from previous time unit - $\\bar{s}_{t}$. Weight matrices stayes as they were (I mean in terms of sizes and connections; during training actual weights are being updated).<br>\n",
    "\n",
    "To avoid messy graphs, **unfolded model** can be used to present much bigger and complex RNN's.<br>\n",
    "\n",
    "Simple unfolded model | More complex unfloded model\n",
    "- | - \n",
    "![alt](images/RNN7.png) | ![alt](images/RNN8.png)\n",
    "\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "In the model on the right we can see that output $\\bar{y}_{t}$ actually is the input for the next layer $\\bar{O}_{t}$. That way we can easliy visualize how additional layers are stack up on each other creating more complex and dense architectures.\n",
    "\n",
    "### Training RNN - BackPropagation Through Time (BPTT)\n",
    "\n",
    "RNN takes into account not only current output but also previous ones. Due to that process of calculating gradient has to do the same. **Backpropagation Through time (BPTT)** calculates gradients in respect of previous states.<br>\n",
    "\n",
    "Through **unfolded model** one can show how exacly those gradients are computed. Let's take for example architecture presented below. <br>\n",
    "\n",
    "<img src=\"images/RNN10.png\" width=\"300\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "Task is to calculate the update rule for weight matrix $U$ at time $t+1$ assuming the error function is detenoded as $E$. In the above image, it can be seen that presented model has 2 time steps: $t$ and $t+1$.<br>\n",
    "Finding that update rule is the same as answering the question: **What are all paths coming from $\\bar{y}_{t+1}$ to $\\{ \\bar{x}_{t}, \\bar{x}_{t+1} \\}$?**.<br>\n",
    "\n",
    "In the above image **first** path is visible. Below second and third are presented.<br>\n",
    "\n",
    "Second path | Third path\n",
    "- | - \n",
    "![alt](images/RNN11.png) | ![alt](images/RNN9.png)\n",
    "\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "After considering all 3 paths, we can come up with final update rule for $U$ matrix. It consists of 3 terms which corresponds to **first, second** and **third** path.<br>\n",
    "\n",
    "<img src=\"images/RNN12.png\" width=\"400\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "### Problems connected with BPTT\n",
    "\n",
    "During training RNN's, BPTT can be used to calculate gradients of error function and update matrices weghts. One can decide to use Mini-Batch training, so not to update weights on every step but after processing each batch of fixed size.<br>\n",
    "\n",
    "In that approach, gradients are accumulated and then averaged in order to obtain update rule after each batch processed. Backpropagating to many times causes **gradient to vanish**, that is information decays geometrically over time. To address that problem **Long Short-Term Memory(LSTM)** model were introduced on which I elaborate in this [section](LSTM.ipynb).<br>\n",
    "\n",
    "Also, in RNN's there is the opposite problem. **Exploding gradient** problem occurs when gradients grows without control. To address that, simple solution were introduced called **Gradient Clipping** on which more information can be found [here](https://arxiv.org/pdf/1211.5063.pdf) under section _3.2. Scaling down the gradients_ (Algorithm 1).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
