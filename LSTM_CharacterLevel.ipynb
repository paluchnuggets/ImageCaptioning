{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook I will train character-level LSTM. The model will train character by character on some text in order to aftterewards produce completely new text also character by character. This example will be based on Anna Karenina text. The goal is to produce a network that will be able to generate a chunk of text based in the same style as Anna Karenina.<br>\n",
    "\n",
    "The structure of the network.<br>\n",
    "<img src=\"images/LSTM4.jpeg\" width=\"500\"><br>\n",
    "Credits: Udacity Computer vision Nanodegree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding and preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load resources\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open text file and read it as a text\n",
    "with open(\"data/anna.txt\", \"r\") as stream:\n",
    "    text = stream.read()\n",
    "\n",
    "# show sample\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to map that text into integers and to do that following steps were taken:\n",
    "1. Create a tuple of all distinct characters preent in the text\n",
    "2. From that tuple create a dictionary where integers are keys\n",
    "3. Then inverse mapping from relation `int : char` to `char: int`\n",
    "4. Using dict from point 3. map whole text into a list of corresponding integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: use set constructor to get unique chars from whole text. Next make it immutable using tuple\n",
    "chars = tuple(set(text))\n",
    "# step 2: create dictionary `int : char`\n",
    "int2char = dict(enumerate(chars))\n",
    "# step 3: now inverse the mapping\n",
    "char2int = {char : integer for integer, char in int2char.items()}\n",
    "# step 4: map every char in text to the corresponding value in char2int dict, save it as numpy array\n",
    "encoded = np.array([char2int[char]for char in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see again the first line of the text and its encoded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 47, 46,  3, 82, 49,  1,  8,  6, 79, 79, 79, 10, 46,  3,  3,  0,\n",
       "        8, 51, 46, 65, 53, 26, 53, 49, 24,  8, 46,  1, 49,  8, 46, 26, 26,\n",
       "        8, 46, 26, 53, 55, 49, 72,  8, 49, 78, 49,  1,  0,  8, 21, 12, 47,\n",
       "       46,  3,  3,  0,  8, 51, 46, 65, 53, 26,  0,  8, 53, 24,  8, 21, 12,\n",
       "       47, 46,  3,  3,  0,  8, 53, 12,  8, 53, 82, 24,  8, 40, 13, 12, 79,\n",
       "       13, 46,  0, 67, 79, 79, 16, 78, 49,  1,  0, 82, 47, 53, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of distinct characters in our text == size of our vocabulary\n",
    "max(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is working perfectly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "As can be seen on the image above LSTM expects one-hot encoded characters. So for every letter we would have a vector of length `max(encoded) = 82`, where only one element will be 1 representing that particular character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making training mini-batches\n",
    "When making mini-batches in sequence data it is important to understand wheter we are talking abiut **batch size** or **sequence lenght**. Image presented below perfectly describes that.<br>\n",
    "\n",
    "<img src=\"images/LSTM5.png\" width=\"500\"><br>\n",
    "Credits: Udacity Computer vision Nanodegree<br>\n",
    "\n",
    "### Creating batches: step-by-step guide\n",
    "Legend:<br>\n",
    "`N - batch size`<br>\n",
    "`M - Sequence length`<br>\n",
    "`K - Total number of completely full batches of size N`<br>\n",
    "`arr - sequence of encoded characters(ecoded by a dictionary, not one-hot encoded)`<br>\n",
    "`n - number of all characters in arr, simply len(arr)`\n",
    "\n",
    "1. Discard data that do not fit in complete batches<br>\n",
    "To do taht we need to compute `K`. It is simply number of all chars in `arr`, `n` divided by number of chars in a single batch `N * M`. Once we get `K` we have to multiply it by `N * M` in order to obtain number of chars from `arr` we want to keep.\n",
    "\n",
    "2. Having prepared `arr` we need to split it into `N` sequences\n",
    "`arr` has to be reshaped into matrix of size `(N, M * K)`.\n",
    "\n",
    "3. Lastly, we have to iterate through that matrix to get our batches\n",
    "Iterate through that matrix can be see as moving a window of size `(N, M)` with a step `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch\n",
      " [[41 47 46  3 82 49  1  8  6 79]\n",
      " [ 0 52 63  8 46 12 24 13 49  1]\n",
      " [46 61 12 53 51 53 37 49 12 82]\n",
      " [ 8 46  8  3  1 49 78 53 40 21]\n",
      " [49  8 82 47 46 12 55 49 19  8]\n",
      " [ 8 24 46 13  8 47 49  1  8 82]\n",
      " [37 82 49 19  8 13 47 46 82 79]\n",
      " [46 24  8 24 21 51 51 49  1 53]\n",
      " [49 19  8 53 82 24  8 82 53 65]\n",
      " [37 49  8 40 51  8 65 53 12 19]]\n",
      "Target batch\n",
      " [[47 46  3 82 49  1  8  6 79 79]\n",
      " [52 63  8 46 12 24 13 49  1 49]\n",
      " [61 12 53 51 53 37 49 12 82 72]\n",
      " [46  8  3  1 49 78 53 40 21 24]\n",
      " [ 8 82 47 46 12 55 49 19  8 74]\n",
      " [24 46 13  8 47 49  1  8 82 49]\n",
      " [82 49 19  8 13 47 46 82 79 47]\n",
      " [24  8 24 21 51 51 49  1 53 12]\n",
      " [19  8 53 82 24  8 82 53 65 49]\n",
      " [49  8 40 51  8 65 53 12 19  8]]\n"
     ]
    }
   ],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generator that returs batches of size (n_seqs, n_steps) from arr\n",
    "    \n",
    "    Paramteters:\n",
    "    -----------\n",
    "    arr: numpy array from which the batches are created\n",
    "    n_seqs: batch size (N)\n",
    "    n_steps: num (M)\n",
    "    \n",
    "    \"\"\"\n",
    "    # compute number of characters in a batch (K)\n",
    "    num_char_batch = n_seqs * n_steps\n",
    "    \n",
    "    # get the number of batches that fit arr completely, // - integer division\n",
    "    n_batches = len(arr)//num_char_batch\n",
    "    \n",
    "    # keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*num_char_batch]\n",
    "    \n",
    "    # reshape arr in order to get shape (N, M*K)\n",
    "    arr = arr.reshape(n_seqs, -1)\n",
    "    \n",
    "    # get batches from prepared array\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # batch with features\n",
    "        x = arr[ : , n : n + n_steps]\n",
    "        \n",
    "        # batch with targets\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        # shift feature batch by one,\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
    "        except IndexError:\n",
    "            #when we get to the end of the batch take first column of the arr\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y        \n",
    "        \n",
    "# Let's test it\n",
    "batches = get_batches(encoded, 10, 10)\n",
    "x, y = next(batches)\n",
    "print(\"Feature batch\\n\",x)\n",
    "print(\"Target batch\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function seems to work as expected. We can see that the second column of the `Feature batch` is the first column of the `Target batch` and also last column in `Feature batch` is the last but one column in `Target batch`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
