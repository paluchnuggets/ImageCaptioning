{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does memory cell works?\n",
    "At time $t$, we receive input $x_t$ (see the image below). Also we get $LTM_{t-1}$ and $STM_{t-1}$ which are vectors of size $n$. <br>\n",
    "### LTM update\n",
    "So first we have to decide which parts of existing $LTM_{t-1}$ we want to keep and which to discard. To do that we can use $STM_{t-1}$ and $x_t$. General thought is that we can learn how to remember/forget certain parts of previous memory states.<br>\n",
    "\n",
    "$remember_{t}=\\sigma{(W_{r}x_{t}+U_{r}STM_{t-1})}$<br>\n",
    "\n",
    "Sigmoid function returns value between 0 and 1 which can be interpreted as how much we should remember certain value: 1 means remember, 0 means forget.<br>\n",
    "\n",
    "Next we have to compute the candidate for $LTM_{t}$ based on current input and $STM_{t-1}$.<br>\n",
    "\n",
    "$LTM^{'}_{t}=\\phi{(W_{l}x_{t}+U_{l}STM_{t-1})}$, where $\\phi$ is the activation function - here, **hiberbolic tangens** is commonly used.<br>\n",
    "\n",
    "After that, we need to decide what part of that new knowledge is worth saving, so we do that by computing **save gate** which is similar to **remember gate**, but serves different purpose (and has it own weight matrices).<br>\n",
    "\n",
    "$save_{t}=\\sigma{(W_{s}x_{t}+U_{s}STM_{t-1})}$<br>\n",
    "\n",
    "Now we can combine all that to finally compute $LTM_{t}$.<br>\n",
    "\n",
    "$LTM_{t} = remember_{t} \\circ LTM_{t-1} + save_{t} \\circ LTM^{'}_t$, where $\\circ$ detones element-wise multiplication.<br>\n",
    "\n",
    "Basically the equation above states: Forget what is no longer relevant, retain what is, learn only important part of incoming information.<br>\n",
    "\n",
    "### STM update\n",
    "After calculating $LTM_{t}$ we need to take care of computing $STM_{t}$. To do that we use the same logic as during $LTM$ updates. Gate **focus** need to be learned to determine which part of $LTM_{t}$ need to be used immidiately, in that concrete moment $t$.<br>\n",
    "\n",
    "$focus_{t}=\\sigma{(W_{f}x_{t}+U_{f}STM_{t-1})}$<br>\n",
    "$STM_{t}=focus_{t} \\circ \\phi{(LTM_{t})}$<br>\n",
    "\n",
    "Comparing to RNN's, LSTM's have very sophisticated way of processing the input using *memory* component.<br>\n",
    "\n",
    "<img src=\"images/LSTM1.png\"><br>\n",
    "Credits: Edwin Chen's [blog](http://blog.echen.me/2017/05/30/exploring-lstms/)<br>\n",
    "\n",
    "And RNN structure for comparition.<br>\n",
    "\n",
    "<img src=\"images/LSTM2.png\"><br>\n",
    "Credits: Edwin Chen's [blog](http://blog.echen.me/2017/05/30/exploring-lstms/)<br>\n",
    "\n",
    "### Notation\n",
    "This theoretical description of LSTM is based on brilliant Edwin Chen's [blog post](http://blog.echen.me/2017/05/30/exploring-lstms/). He uses there specific notation which is quite different then that used in literature. Due to that a little dictionary has to be provided. <br>\n",
    "\n",
    "- The $LTM_{t}$, is usually called the **cell state**, denoted $c_{t}$.\n",
    "- The $STM_{t}$, is usually called the **hidden state**, denoted $h_{t}$. This is analogous to the hidden state in vanilla RNNs.\n",
    "- The $remember_{t}$, is usually called the **forget gate** (despite the fact that a 1 in the forget gate still means to keep the memory and a 0 still means to forget it), denoted $f_{t}$.\n",
    "- The $save_{t}$, is usually called the **input gate** (as it determines how much of the input to let into the cell state), denoted $i_{t}$.\n",
    "- The $focus_{t}$, is usually called the **output gate**, denoted $o_{t}$. )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
