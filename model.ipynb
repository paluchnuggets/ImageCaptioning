{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # store important shapes.sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # embedding layer which converts vectors to embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer which takes embedded vector as input\n",
    "        # and outputs hidden state of size hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=embed_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully-connected layer maps LSTM output into vocab_size \n",
    "        self.hidden2vocab = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        # first we have to pre-process features and captions in order to concatenate it into one tensor\n",
    "        \n",
    "        # from (batch_size, caption_len_in_current_batch) -> (batch_size, cap_len_cur_btch-1, embed_size)\n",
    "        # last column is ommited because we don't want our model to predict next word when <end> is input\n",
    "        captions = self.embed(captions[:,:-1]) \n",
    "        # from (batch_size, embed_size) -> (batch_size, 1, embed_size)\n",
    "        features = features.unsqueeze(dim=1)\n",
    "        \n",
    "        # now we can concat those tensors in order to obtain input tensor\n",
    "        # dim=1 means that we concatenate along horizontal axis\n",
    "        # shape: (batch_size, cap_len_cur_btch -1 + 1, embed_size)\n",
    "        inputs = torch.cat((features, captions), dim=1)\n",
    "        \n",
    "        # having inputs concatenated we process them through our network\n",
    "        # from inputs.shape -> (batch_size, cap_len_cur_btch, hidden_size)\n",
    "        lstm_output, _ = self.lstm(inputs)\n",
    "        \n",
    "        # then fully-connected one\n",
    "        # from lstm_output.shape -> (batch_size, cap_len_cur_btch, vocab_size)\n",
    "        outputs = self.hidden2vocab(lstm_output)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def init_hidden(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state of LSTM: (short term memonry, long-term memory) of shape\n",
    "        (num_layers, batch_size, hidden_size) each\n",
    "        \n",
    "        ------------------------------------------\n",
    "        Parameters:\n",
    "        hidden_size: size of the hidden layer in LSTM model\n",
    "        \"\"\"\n",
    "        \n",
    "        return (torch.zeros((1, batch_size, self.hidden_size)), torch.zeros((1, batch_size, self.hidden_size)))\n",
    "        \n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"\n",
    "        Accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len).\n",
    "        \n",
    "         ------------------------------------------\n",
    "        Parameters:\n",
    "        inputs: torch tensor of size (1, 1, embed_size) representing embedded single image\n",
    "        max_len: lenght of the caption generated for image provided in inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = (torch.randn(self.num_layers, 1, self.hidden_size).to(inputs.device), torch.randn(self.num_layers, 1, self.hidden_size).to(inputs.device))\n",
    "        \n",
    "        caption = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            lstm_output, hidden = self.lstm(inputs, hidden) # inputs.shape = (1, 1, embed) -> (1, 1, hidden_size)\n",
    "            outputs = self.hidden2vocab(lstm_output) # (1, 1, hiden_size) -> (1, 1, vocab_size)\n",
    "            \n",
    "            # first from (1, 1, vocab_size) -> (1, vocab_size) then return index of the most probable token in vocab\n",
    "            outputs = outputs.squeeze(1)\n",
    "            \n",
    "            #pred word\n",
    "            pred_word = outputs.argmax(dim=1)\n",
    "            # after that we append prediction to the caption list\n",
    "            # .item() convert torch tensor with that maximum index to integer value\n",
    "            caption.append(pred_word.item())\n",
    "            \n",
    "            # prepare output from timestep t to be an input for timestep t+1\n",
    "            # from (1, 1) to (1, 1, embed) again\n",
    "            inputs = self.embed(pred_word.unsqueeze(0))\n",
    "        \n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2207,  0.1635, -0.8529,  0.8501]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,1,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.squeeze_(1).argmax(dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.argmax(dim=1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8708, 0.3423, 0.3274, 1.5625]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
