{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoders and Decoders in Sequence to Sequence models and Attention Mechanisms purpose.\n",
    "In Sequence to Sequence models our input is the sequence of elements (e.g words) and the output is also sequence of elements.<br>\n",
    "\n",
    "Machine translation example<br>\n",
    "<img src=\"images/AM1.png\" width=\"600\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "Generally, **encoder**  turn all inputs into one vector called **Context Vector** which encoder was able to capture from input sequence. Then that vector is passed to the **encoder** which produces output sequence.<br><br><br>\n",
    "**Example**: In Machine Translation, encoder and decoder are both RNN, often with LSTM cells. The whole process looks as follows(image above):<br><br>\n",
    "Let's assume that `word1 = comment`, `word2 = allez`, `word3 = vous`:<br><br>\n",
    "We process `word1` with Encoder, which outputs `HiddenState1`, then using as inputs `word2` and `HiddenState1` we produce `HiddenState2`. After that we take `word3` and `HiddenState2` as an input and process that with Encoder and get `HiddenState3` which becames our **Context Vector**. Our Context Vector `(HiddenState3)` is an input for Decoder which produce an output consists of 3 words translated to specific language.<br>\n",
    "\n",
    "In that approach there is one serious limitation. The size of our **Context Vector** is constatnt, disregarding lenght of our input size. One could say that enlarging size of the Context Vector would address this problem but then with small input sequences problem of **overfittting** occurs. To solve that problem **Attention Mechanism** were introduced!\n",
    "\n",
    "# Sequence to Sequence model with Attention\n",
    " \n",
    "Attention in Encoder<br>\n",
    "<img src=\"images/AM2.png\" width=\"600\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "Process look very simmilar to the previous one but here instead of taking only last hidden state, we take all of them. After that a t each timestep **Attention Decoder** decides which vector should be used as an input, so it process it one by one but not necessarily sequentially. Decoder is trained during training on which vector should it be focused on next. Below, very interesting example is presented.<br>\n",
    "\n",
    "Translation of sentence in French to English<br>\n",
    "<img src=\"images/AM3.png\" width=\"600\"><br>\n",
    "Credits: Udacity Computer Vision Nanodegree<br>\n",
    "\n",
    "**Assumptions**: Behaviour presented above is presented on the **trained model**. It should be unterstood as follows.<br>\n",
    "\n",
    "For each word in pink box, the squares in corresponding column addresses attention that was paid to the words presented in green box (the brigter the more attention was paid). So in order to get second word `agreement`, Decoder paid great attention to the vector representing word `accord` and a little attention to the word `L'`. What is intertesting about it is that until fourth word `la`, the process went sequentailly. Then Decoder decided to translate word `europeenne`, so it jumped over 2 words!. That was because of differences between the order of words in those two languages. Decoder with attention was able to capture that dependency (that is amazing in my personal opinion).<br>\n",
    "\n",
    "### Attention Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
